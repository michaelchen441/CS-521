{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Whsg1XX_OZs6"
   },
   "source": [
    "# Boilerplate\n",
    "\n",
    "Package installation, loading, and dataloaders. There's also a simple model defined. You can change it your favourite architecture if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R1domTvnONqD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GloVe\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchtext\n",
    "import torchdata\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 32\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        self.fc = nn.Linear(50, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# class Normalize(nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return (x - 0.1307)/0.3081\n",
    "\n",
    "# Add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "# model = Net()#nn.Sequential(Normalize(), Net())\n",
    "\n",
    "# model = model.to(device)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Mja_AB4RykO"
   },
   "source": [
    "# Implement Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_case_logits(data, label, model, eps_test):\n",
    "    lb = data - eps_test\n",
    "    ub = data + eps_test\n",
    "    \n",
    "    lb = model.embedding(lb.long())\n",
    "    ub = model.embedding(ub.long())\n",
    "\n",
    "    lb = lb.mean(dim=1)\n",
    "    ub = ub.mean(dim=1)\n",
    "\n",
    "    #propogate through each layer based on linear or ReLU\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Embedding):\n",
    "            continue\n",
    "        lb = layer(lb)\n",
    "        ub = layer(ub)\n",
    "\n",
    "    dim = lb.shape[-1]\n",
    "    true_classes = label.unsqueeze(-1) == torch.arange(dim).to(device)\n",
    "    # print(\"true classes\", true_classes)\n",
    "    # print(true_classes.shape, lb.shape)\n",
    "    logits = torch.where(true_classes, lb, ub)\n",
    "    # print(\"logits\", logits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_IBP(model, num_epochs, train_loader):\n",
    "    #  trains a given model on the MNIST dataset.\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # setup eps_train and k steps for each epoch\n",
    "    eps_test = 0 \n",
    "    eps_test_step = .1 / num_epochs\n",
    "    \n",
    "    k = 1\n",
    "    k_step = -0.5 / num_epochs\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for data, label in train_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            z_hat = get_worst_case_logits(data, label, model, eps_test)\n",
    "\n",
    "            #TODO edit loss function\n",
    "            loss_fit = F.cross_entropy(out, label)\n",
    "            loss_spec = F.cross_entropy(z_hat, label) \n",
    "            # print(\"loss_fit:\", loss_fit, \"loss_spec:\", loss_spec)\n",
    "            loss = (k*loss_fit) + ((1-k)*loss_spec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        eps_test += eps_test_step\n",
    "        k += k_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interval analysis\n",
    "def interval_analysis(model, input, eps):\n",
    "\n",
    "    lb = input - eps\n",
    "    ub = input + eps\n",
    "\n",
    "    lb = torch.clamp(lb, 0, 1)\n",
    "    ub = torch.clamp(ub, 0, 1)\n",
    "    \n",
    "    lb_out = model(lb)\n",
    "    ub_out = model(ub)\n",
    "\n",
    "    return lb_out, ub_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    # TODO: implement this function to test the robust accuracy of the given model\n",
    "    # use pgd_untargeted() within this function\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for data, label in test_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        out = model(data)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        \n",
    "        # print(label.size(0))\n",
    "        # print(data.size(0))\n",
    "        # print(out.size(0))\n",
    "        total += label.size(0)\n",
    "        # print(predicted)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "    print(\"accuracy\", 100 * correct / total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustness(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for eps in [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]:\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out_lb, out_ub = interval_analysis(model, data, eps)\n",
    "            out_lb, out_ub = out_lb.argmax(dim=1), out_ub.argmax(dim=1) #choose class for each image\n",
    "            \n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (out_lb == out_ub).sum().item()\n",
    "        print(\"eps:\", eps)\n",
    "        print(\"percent robust\", 100 * correct / total)\n",
    "        print()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPMdfEhtR3zm"
   },
   "source": [
    "# Study Accuracy, Quality, etc.\n",
    "\n",
    "Compare the various results and report your observations on the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last argument 'targeted' can be used to toggle between a targeted and untargeted attack.\n",
    "def fgsm(model, x, y, eps):\n",
    "    #TODO: implement this as an intermediate step of PGD\n",
    "    # Notes: put the model in eval() mode for this function\n",
    "    model.eval()                   \n",
    "\n",
    "    x.requires_grad = True\n",
    "\n",
    "    #get gradient loss\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output,y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    sign_x = x.grad.sign()\n",
    "\n",
    "    #get eta with e * sign(loss grad) \n",
    "    n = eps * sign_x\n",
    "\n",
    "    x_prime = x + n\n",
    "    x_prime = torch.clamp(x_prime,0,1)\n",
    "\n",
    "    return x_prime\n",
    "\n",
    "def pgd_untargeted(model, x, y, k, eps, eps_step):\n",
    "    #TODO: implement this \n",
    "    # Notes: put the model in eval() mode for this function\n",
    "    # x: input image\n",
    "    # y: ground truth label for x\n",
    "    # k: steps of FGSM\n",
    "    # eps: projection region for PGD (note the need for normalization before projection, as eps values are for inputs in [0,1])\n",
    "    # eps_step: step for one iteration of FGSM\n",
    "    model.eval()\n",
    "\n",
    "    x_init = x.clone().detach()\n",
    "    ball_max = x_init + eps\n",
    "    ball_min = x_init - eps\n",
    "\n",
    "    for _ in range(k):\n",
    "\n",
    "        x.requires_grad = True\n",
    "        x_new = fgsm(model, x, y, eps_step)\n",
    "\n",
    "        x_new = torch.clamp(x_new, ball_min, ball_max)\n",
    "        x_new = torch.clamp(x_new, 0, 1)\n",
    "\n",
    "        x = x_new.detach()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_attacks(model, attack='pgd', k=10, eps=0.1):\n",
    "    # TODO: implement this function to test the robust accuracy of the given model\n",
    "    # use pgd_untargeted() within this function\n",
    "\n",
    "    eps_step = eps/k\n",
    "    model.eval()\n",
    "\n",
    "    correct, correct_second, total = 0, 0, 0\n",
    "\n",
    "    for data, label in test_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        if attack == 'pgd':\n",
    "            #TODO implement\n",
    "            adversarial_data = pgd_untargeted(model, data, label, k, eps, eps_step)\n",
    "\n",
    "            full_data = torch.cat((data, adversarial_data))\n",
    "            label = torch.cat((label, label))\n",
    "        else:\n",
    "            full_data = data\n",
    "\n",
    "        out = model(full_data)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        \n",
    "        # print(label.size(0))\n",
    "        # print(data.size(0))\n",
    "        # print(out.size(0))\n",
    "        total += label.size(0)\n",
    "        # print(predicted)\n",
    "        correct += (predicted[:data.size(0)] == label[:data.size(0)]).sum().item()\n",
    "        if attack == 'pgd':\n",
    "            correct_second += (predicted[data.size(0):] == label[:data.size(0)]).sum().item()\n",
    "\n",
    "    if attack == 'pgd':\n",
    "        print(\"for eps\", eps)\n",
    "        print(\"robust accuracy\", 100 * (correct+correct_second) / total)\n",
    "        print(\"standard accuracy\", 100 * 2*correct / total)\n",
    "        print(\"adversarial accuracy\", 100 * 2*correct_second / total)\n",
    "    else:\n",
    "        print(\"accuracy\", 100 * correct / total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interval analysis\n",
    "def interval_analysis(model, input, eps):\n",
    "\n",
    "    lb = input - eps\n",
    "    ub = input + eps\n",
    "\n",
    "    lb = torch.clamp(lb, 0, 1)\n",
    "    ub = torch.clamp(ub, 0, 1)\n",
    "    \n",
    "    lb_out = model(lb)\n",
    "    ub_out = model(ub)\n",
    "\n",
    "    return lb_out, ub_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustness(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for eps in [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]:\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            out_lb, out_ub = interval_analysis(model, data, eps)\n",
    "            out_lb, out_ub = out_lb.argmax(dim=1), out_ub.argmax(dim=1) #choose class for each image\n",
    "            \n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (out_lb == out_ub).sum().item()\n",
    "        print(\"eps:\", eps, \"percent robust\", 100 * correct / total)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#SETUP test 2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenizer and vocabulary setup\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "#SETUP\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter = collections.Counter()\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "for label, line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "counter['<unk>'] = 1\n",
    "counter['<pad>'] = 1\n",
    "\n",
    "vocab_obj = vocab(counter)\n",
    "vocab_obj.set_default_index(vocab_obj['<unk>'])\n",
    "glove = GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "vocab_size = len(vocab_obj)\n",
    "embedding_dim = glove.dim\n",
    "pretrained_embeddings = torch.zeros(vocab_size, embedding_dim)\n",
    "for i, token in enumerate(vocab_obj.get_itos()):\n",
    "    if token in glove.stoi:\n",
    "        pretrained_embeddings[i] = glove[token]\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.data = list(IMDB(split=split))\n",
    "        self.vocab = vocab_obj\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, text = self.data[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        token_ids = [self.vocab[token] for token in tokens[:max_length]]\n",
    "        # Pad if shorter than max_length\n",
    "        if len(token_ids) < max_length:\n",
    "            token_ids.extend([self.vocab['<pad>']] * (max_length-len(token_ids)))\n",
    "        label = 1 if label == 'pos' else 0\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = IMDBDataset(split='train')\n",
    "test_dataset = IMDBDataset(split='test')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model = Net(pretrained_embeddings)\n",
    "model = model.to(device)\n",
    "\n",
    "start = time.time()\n",
    "train_model_IBP(model, 15, train_loader)\n",
    "end = time.time()\n",
    "print(\"IBP train time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torchtext) (2.31.0)\n",
      "Collecting torch==2.2.2 (from torchtext)\n",
      "  Using cached torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torchtext) (1.24.4)\n",
      "Requirement already satisfied: filelock in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from torch==2.2.2->torchtext) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from requests->torchtext) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch==2.2.2->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages (from sympy->torch==2.2.2->torchtext) (1.3.0)\n",
      "Using cached torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of torch: [Errno 2] No such file or directory: '/Users/Michael/opt/anaconda3/lib/python3.8/site-packages/torch-2.2.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: No metadata found in /Users/Michael/opt/anaconda3/lib/python3.8/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 2.2.1\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall torch 2.2.1\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for torch.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: You might be able to recover from this via: \u001b[32mpip install --force-reinstall --no-deps torch==2.2.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install ruamel_yaml==0.11.14\n",
    "!pip install torchtext\n",
    "# import torch\n",
    "# import torchtext\n",
    "# !pip install torchdata\n",
    "# import torchdata\n",
    "# !conda update conda -y\n",
    "# !pip install torchtext==0.9.0\n",
    "# !pip install torch==1.13.1\n",
    "# !pip show torchtext\n",
    "# !pip install --force-reinstall --no-deps torch==2.2.1\n",
    "# !pip install --upgrade torch\n",
    "\n",
    "\n",
    "# !pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train the original model\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "start = time.time()\n",
    "train_model_IBP(model, 15)\n",
    "end = time.time()\n",
    "print(\"IBP train time: \", end - start)\n",
    "\n",
    "torch.save(model.state_dict(), 'weights_IBP2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(Normalize(), Net())\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "start = time.time()\n",
    "train_model(model, 15)\n",
    "end = time.time()\n",
    "print(\"standard train time: \", end - start)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 89.91\n",
      "accuracy 89.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = Net(pretrained_embeddings)\n",
    "model.load_state_dict(torch.load('weights_IBP2.pt'))\n",
    "\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## robust test\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model.load_state_dict(torch.load('weights.pt'))\n",
    "\n",
    "for eps in [.05]:# [0.05, 0.1, 0.15, 0.2]:\n",
    "    test_model_on_attacks(model, attack='pgd', k=10, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## robust test\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "model.load_state_dict(torch.load('weights_IBP.pt'))\n",
    "\n",
    "for eps in [.05]:# [0.05, 0.1, 0.15, 0.2]:\n",
    "    test_model_on_attacks(model, attack='pgd', k=10, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robustness test\n",
    "test_robustness(model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
